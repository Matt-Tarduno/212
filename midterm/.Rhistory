sim_df <- parLapply(
cl = cl,
X = 1:1000,
fun = one_run,
population = data_df, size=n) %>% bind_rows() %>% tbl_df()
# Plotting ----
# Plot a separate histogram of the estimated β2’s for each sample size:
plot_data<-subset(sim_df,  param == "b2")
plot<-ggplot(plot_data, aes(est)) +
geom_histogram(aes(y = ..density..), binwidth = 1/(n^(2/3))) +
stat_function(fun = dnorm, args = list(mean = -2, sd = 1/sqrt(n)))
plot
print(plot)
}
set.seed(22092008)
sample_size=100000
data_df <- data.frame(
i = 1,
x1 = rnorm(sample_size, 0, 5),
x2 = rnorm(sample_size, 0, 5),
e = rnorm(sample_size, 0, 5),
eta = runif(sample_size, -8.66, 8.66))
# Calculate y = 7 + 0.5 x + e; drop 'e'
data_df %<>% mutate(y_a = 3 + 1*x1  - 2*x2 + e)
data_df %<>% mutate(y_b = 3 + 1*x1  - 2*x2 + eta)
# Functions ----
# Function to convert tibble, data.frame, or tbl_df to matrix
to_matrix <- function(the_df, vars) {
# Create a matrix from variables in var
new_mat <- the_df %>%
# Select the columns given in 'vars'
select_(.dots = vars) %>%
# Convert to matrix
as.matrix()
# Return 'new_mat'
return(new_mat)
}
# Function for OLS coefficient estimates
b_ols <- function(y, X) {
# Calculate beta hat
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
# Return beta_hat
return(beta_hat)
}
# Function for a single iteration of the simulation
one_run <- function(iter, population, size) {
sample_df <- sample_n(tbl = population, size)
# Calculate the OLS coef. (using unweighted variables)
coef_ols <- b_ols(
y = to_matrix(sample_df, "y_a"),
X = to_matrix(sample_df, c("i", "x1", "x2")))
# Create a data.frame to return
coef_df <- data.frame(
est    = as.vector(coef_ols),
param  = c("int", "b1", "b2"),
iter   = iter
)
# Return the data.frame
print(coef_ols)
return(coef_df)
}
# Make the cluster
cl <- makeCluster(4)
# Load functions on the cluster
clusterEvalQ(cl, {
library(dplyr)
library(magrittr)
})
# Export our data and functions to the cluster
clusterExport(cl, "data_df")
clusterExport(cl, c("to_matrix", "b_ols", "one_run"))
# Set seed in parallel
clusterSetRNGStream(cl, 12345)
# this is for sample size 10:
# for this sample size I:
#     - Have my population
#     - Draw a sample size of 10 from my population, at random, 1000 times:
# repeating for other sample sizes:
n_list<-c(10, 100, 1000, 10000, 20000)
for (n in n_list) {
sim_df <- parLapply(
cl = cl,
X = 1:1000,
fun = one_run,
population = data_df, size=n) %>% bind_rows() %>% tbl_df()
# Plotting ----
# Plot a separate histogram of the estimated β2’s for each sample size:
plot_data<-subset(sim_df,  param == "b2")
plot<-ggplot(plot_data, aes(est)) +
geom_histogram(aes(y = ..density..), binwidth = 1/(n^(2/3))) +
stat_function(fun = dnorm, args = list(mean = -2, sd = 1/sqrt(n)))
plot
print(plot)
}
# repeating for other sample sizes:
n_list<-c(10, 100, 1000, 10000, 20000)
for (n in n_list) {
sim_df <- parLapply(
cl = cl,
X = 1:1000,
fun = one_run,
population = data_df, size=n) %>% bind_rows() %>% tbl_df()
# Plotting ----
# Plot a separate histogram of the estimated β2’s for each sample size:
plot_data<-subset(sim_df,  param == "b2")
plot<-ggplot(plot_data, aes(est)) +
geom_histogram(aes(y = ..density..), binwidth = 1/(n^(2/3))) +
stat_function(fun = dnorm, args = list(mean = -2, sd = 1/sqrt(n))) + tarduno_theme_small
plot
print(plot)
}
# theme ----
tarduno_theme_small <- theme_set(theme_classic())+
theme(text=element_text(size=14, colour="#7a8598"),
plot.title = element_text(hjust = 0.5))
# repeating for other sample sizes:
n_list<-c(10, 100, 1000, 10000, 20000)
for (n in n_list) {
sim_df <- parLapply(
cl = cl,
X = 1:1000,
fun = one_run,
population = data_df, size=n) %>% bind_rows() %>% tbl_df()
# Plotting ----
# Plot a separate histogram of the estimated β2’s for each sample size:
plot_data<-subset(sim_df,  param == "b2")
plot<-ggplot(plot_data, aes(est)) +
geom_histogram(aes(y = ..density..), binwidth = 1/(n^(2/3))) +
stat_function(fun = dnorm, args = list(mean = -2, sd = 1/sqrt(n))) + tarduno_theme_small
plot
print(plot)
}
tarduno_theme <- theme(
legend.position = "bottom",
panel.background = element_rect(fill = NA),
panel.border = element_rect(fill = NA, color = "grey75"),
axis.ticks = element_line(color = "grey85"),
panel.grid.major = element_line(color = "grey95", size = 0.2),
panel.grid.minor = element_line(color = "grey95", size = 0.2),
legend.key = element_blank())
# repeating for other sample sizes:
n_list<-c(10, 100, 1000, 10000, 20000)
for (n in n_list) {
sim_df <- parLapply(
cl = cl,
X = 1:1000,
fun = one_run,
population = data_df, size=n) %>% bind_rows() %>% tbl_df()
# Plotting ----
# Plot a separate histogram of the estimated β2’s for each sample size:
plot_data<-subset(sim_df,  param == "b2")
plot<-ggplot(plot_data, aes(est)) +
geom_histogram(aes(y = ..density..), binwidth = 1/(n^(2/3))) +
stat_function(fun = dnorm, args = list(mean = -2, sd = 1/sqrt(n))) + tarduno_theme
plot
print(plot)
}
# repeating for other sample sizes:
n_list<-c(10, 100, 1000, 10000, 20000)
for (n in n_list) {
sim_df <- parLapply(
cl = cl,
X = 1:1000,
fun = one_run,
population = data_df, size=n) %>% bind_rows() %>% tbl_df()
# Plotting ----
# Plot a separate histogram of the estimated β2’s for each sample size:
title_n<-as.character(n)
plot_data<-subset(sim_df,  param == "b2")
plot<-ggplot(plot_data, aes(est)) +
geom_histogram(aes(y = ..density..), binwidth = 1/(n^(2/3))) +
stat_function(fun = dnorm, args = list(mean = -2, sd = 1/sqrt(n))) +
xlab("Estimate of B2") +
ylab("Density") +
ggtitle(as.character(n))+
tarduno_theme
plot
print(plot)
}
# repeating for other sample sizes:
n_list<-c(10, 100, 1000, 10000, 20000)
for (n in n_list) {
sim_df <- parLapply(
cl = cl,
X = 1:1000,
fun = one_run,
population = data_df, size=n) %>% bind_rows() %>% tbl_df()
# Plotting ----
# Plot a separate histogram of the estimated β2’s for each sample size:
title_n<-as.character(n)
plot_data<-subset(sim_df,  param == "b2")
plot<-ggplot(plot_data, aes(est)) +
geom_histogram(aes(y = ..density..), binwidth = 1/(n^(2/3))) +
stat_function(fun = dnorm, args = list(mean = -2, sd = 1/sqrt(n))) +
xlab("Estimate of B2") +
ylab("Density") +
ggtitle(paste("Sampling distribution for n=", as.character(n))))+
tarduno_theme
plot
print(plot)
}
# repeating for other sample sizes:
n_list<-c(10, 100, 1000, 10000, 20000)
for (n in n_list) {
sim_df <- parLapply(
cl = cl,
X = 1:1000,
fun = one_run,
population = data_df, size=n) %>% bind_rows() %>% tbl_df()
# Plotting ----
# Plot a separate histogram of the estimated β2’s for each sample size:
title_n<-as.character(n)
plot_data<-subset(sim_df,  param == "b2")
plot<-ggplot(plot_data, aes(est)) +
geom_histogram(aes(y = ..density..), binwidth = 1/(n^(2/3))) +
stat_function(fun = dnorm, args = list(mean = -2, sd = 1/sqrt(n))) +
xlab("Estimate of B2") +
ylab("Density") +
ggtitle(paste("Sampling distribution for n=", as.character(n)))+
tarduno_theme
plot
print(plot)
}
# repeating for other sample sizes:
n_list<-c(10, 100, 1000, 10000, 20000)
for (n in n_list) {
sim_df <- parLapply(
cl = cl,
X = 1:1000,
fun = one_run,
population = data_df, size=n) %>% bind_rows() %>% tbl_df()
# Plotting ----
# Plot a separate histogram of the estimated β2’s for each sample size:
plot_data<-subset(sim_df,  param == "b2")
plot<-ggplot(plot_data, aes(est)) +
geom_histogram(aes(y = ..density..), binwidth = 1/(n^(2/3))) +
stat_function(fun = dnorm, args = list(mean = -2, sd = .8/sqrt(n))) +
xlab("Estimate of B2") +
ylab("Density") +
ggtitle(paste("Sampling distribution for n =", as.character(n)))+
tarduno_theme
plot
print(plot)
}
# Part 1
# Setup ----
# Packages
library(pacman)
library(knitr)
# p_load examples
p_load(dplyr, haven, readr, xtable, psych, magrittr)
p_load(ggplot2, extrafont, Matrix)
# Loading data
directory<-"/Users/matthewtarduno/Desktop/212/midterm/"
raw <-read_csv(paste0(directory, "data.csv"), col_types=cols())
# Cleaning Data ----
# Using the dummy.code() function of psych (found using R search function)
dummies<-as.data.frame(dummy.code(data$state))
# Clean dummy names
names(dummies)<-gsub(' ', '\\.', names(dummies))
# Combine the two dataframes
data<-as.data.frame(c(raw, dummies))
#remove % sign
data$emmigrant.ratio<-substr(data$emmigrant.ratio, 1, nchar(data$emmigrant.ratio)-1)
data$emmigrant.ratio<-as.numeric(data$emmigrant.ratio)
data$emmigrant.ratio<-data$emmigrant.ratio/100
# Using the dummy.code() function of psych (found using R search function)
dummies<-as.data.frame(dummy.code(data$state))
# Using the dummy.code() function of psych (found using R search function)
dummies<-as.data.frame(dummy.code(raw$state))
# Clean dummy names
names(dummies)<-gsub(' ', '\\.', names(dummies))
# Combine the two dataframes
data<-as.data.frame(c(raw, dummies))
# Part 1
# Setup ----
# Packages
library(pacman)
library(knitr)
# p_load examples
p_load(dplyr, haven, readr, xtable, psych, magrittr)
p_load(ggplot2, extrafont, Matrix)
# Loading data
directory<-"/Users/matthewtarduno/Desktop/212/midterm/"
raw <-read_csv(paste0(directory, "data.csv"), col_types=cols())
# Cleaning Data ----
# Using the dummy.code() function of psych (found using R search function)
dummies<-as.data.frame(dummy.code(raw$state))
# Clean dummy names
names(dummies)<-gsub(' ', '\\.', names(dummies))
# Combine the two dataframes
data<-as.data.frame(c(raw, dummies))
#remove % sign
data$emmigrant.ratio<-substr(data$emmigrant.ratio, 1, nchar(data$emmigrant.ratio)-1)
data$emmigrant.ratio<-as.numeric(data$emmigrant.ratio)
data$emmigrant.ratio<-data$emmigrant.ratio/100
# Combine the two dataframes
data<-as.data.frame(c(raw, dummies))
View(data)
#remove % sign
data$emmigrant.ratio<-substr(data$emmigrant.ratio, 1, nchar(data$emmigrant.ratio)-1)
# Part 1
# Setup ----
# Packages
library(pacman)
library(knitr)
# p_load examples
p_load(dplyr, haven, readr, xtable, psych, magrittr)
p_load(ggplot2, extrafont, Matrix)
# Loading data
directory<-"/Users/matthewtarduno/Desktop/212/midterm/"
raw <-read_csv(paste0(directory, "data.csv"), col_types=cols())
# Cleaning Data ----
# Using the dummy.code() function of psych (found using R search function)
dummies<-as.data.frame(dummy.code(raw$state))
# Clean dummy names
names(dummies)<-gsub(' ', '\\.', names(dummies))
# Combine the two dataframes
data<-as.data.frame(c(raw, dummies))
#remove % sign
data$emmigrant.ratio<-substr(data$emmigrant.ratio, 1, nchar(data$emmigrant.ratio)-1)
data$emmigrant.ratio
typeof(data$emmigrant.ratio)
#remove % sign
data$emmigrant.ratio<-substr(data$emmigrant.ratio, 1, nchar(as.character(data$emmigrant.ratio))-1)
data$emmigrant.ratio<-as.numeric(data$emmigrant.ratio)
data$emmigrant.ratio<-data$emmigrant.ratio/100
to_matrix <- function(the_df, vars) {
new_mat <- the_df %>%
select_(.dots = vars) %>%
as.matrix()
return(new_mat)
}
# Function for OLS coefficient estimates
b_ols <- function(data, y_var, X_vars, intercept = TRUE) {
require(dplyr)
# Create the y matrix
y <- to_matrix(the_df = data, vars = y_var)
# Create the X matrix
X <- to_matrix(the_df = data, vars = X_vars)
# Intercept
if (intercept == T) {
X <- cbind(1, X)
colnames(X) <- c("intercept", X_vars)
}
# Calculate, return beta hat
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
return(beta_hat)
}
ols(data = data, y = "emmigrant.ratio", X = c("log.corn", "period"))
r2_ols(data = data, y = "emmigrant.ratio", X = c("log.corn", "period"))
lm(data$emmigrant.ratio ~ data$log.corn + data$period)
ols(data = data, y = "emmigrant.ratio", X = c("log.corn.wheat", "period"))
r2_ols(data = data, y = "emmigrant.ratio", X = c("log.corn.wheat", "period"))
lm(data$emmigrant.ratio ~ data$log.corn.wheat + data$period)
# Question 1 Part 5 ----
# 5. What do you think happened here? And what are the consequences?
# They left out the period fixed effects in the peper version of the model.
# ----
# ----
# ----
# Part 2 (Normality of OLS) ----
# yi = βo + β1x1i + β2x2i + εi
#(or replace with eta)
p_load(dplyr, lfe, readr, magrittr, parallel, lfe, ggplot2, ggthemes, viridis)
# βo = 3, β1 = 1, and β2 = −2
# Repeat the following exercise for sample sizes of n=[10, 100, 1000, 10000, 20000]
# For each parameterization of the problem run 1,000 repetitions
# Generate an X matrix with 100,000 elements which has ones in the first column.
# For the second and third column you should generate x1 and x2 from a multivariate
# normal with mean µ=[0,0]. Let each variable have variance 25 and the covariance be zero.
# Make sure that each x hasexactly mean zero (use sweep).
# theme ----
tarduno_theme <- theme(
legend.position = "bottom",
panel.background = element_rect(fill = NA),
panel.border = element_rect(fill = NA, color = "grey75"),
axis.ticks = element_line(color = "grey85"),
panel.grid.major = element_line(color = "grey95", size = 0.2),
panel.grid.minor = element_line(color = "grey95", size = 0.2),
legend.key = element_blank())
# Part A ----
#generate the population
set.seed(22092008)
sample_size=100000
data_df <- data.frame(
i = 1,
x1 = rnorm(sample_size, 0, 5),
x2 = rnorm(sample_size, 0, 5),
e = rnorm(sample_size, 0, 5),
eta = runif(sample_size, -8.66, 8.66))
# Calculate y = 7 + 0.5 x + e; drop 'e'
data_df %<>% mutate(y_a = 3 + 1*x1  - 2*x2 + e)
data_df %<>% mutate(y_b = 3 + 1*x1  - 2*x2 + eta)
# Functions ----
# Function to convert tibble, data.frame, or tbl_df to matrix
to_matrix <- function(the_df, vars) {
# Create a matrix from variables in var
new_mat <- the_df %>%
# Select the columns given in 'vars'
select_(.dots = vars) %>%
# Convert to matrix
as.matrix()
# Return 'new_mat'
return(new_mat)
}
# Function for OLS coefficient estimates
b_ols <- function(y, X) {
# Calculate beta hat
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
# Return beta_hat
return(beta_hat)
}
# Function for a single iteration of the simulation
one_run <- function(iter, population, size) {
sample_df <- sample_n(tbl = population, size)
# Calculate the OLS coef. (using unweighted variables)
coef_ols <- b_ols(
y = to_matrix(sample_df, "y_a"),
X = to_matrix(sample_df, c("i", "x1", "x2")))
# Create a data.frame to return
coef_df <- data.frame(
est    = as.vector(coef_ols),
param  = c("int", "b1", "b2"),
iter   = iter
)
# Return the data.frame
print(coef_ols)
return(coef_df)
}
# Make the cluster
cl <- makeCluster(4)
# Load functions on the cluster
clusterEvalQ(cl, {
library(dplyr)
library(magrittr)
})
# Export our data and functions to the cluster
clusterExport(cl, "data_df")
clusterExport(cl, c("to_matrix", "b_ols", "one_run"))
# Set seed in parallel
clusterSetRNGStream(cl, 12345)
# this is for sample size 10:
# for this sample size I:
#     - Have my population
#     - Draw a sample size of 10 from my population, at random, 1000 times:
# repeating for other sample sizes:
n_list<-c(10, 100, 1000, 10000, 20000)
for (n in n_list) {
sim_df <- parLapply(
cl = cl,
X = 1:1000,
fun = one_run,
population = data_df, size=n) %>% bind_rows() %>% tbl_df()
# Plotting ----
# Plot a separate histogram of the estimated β2’s for each sample size:
plot_data<-subset(sim_df,  param == "b2")
plot<-ggplot(plot_data, aes(est)) +
geom_histogram(aes(y = ..density..), binwidth = 1/(n^(2/3))) +
stat_function(fun = dnorm, args = list(mean = -2, sd = 1/sqrt(n))) +
xlab("Estimate of B2") +
ylab("Density") +
ggtitle(paste("Sampling distribution for n =", as.character(n)))+
tarduno_theme
plot
print(plot)
}
# Part B ----
# Just need to change up what we feed the ols function!
# Also note that the asymptotic variance is also different:
# v is going to be 1/12(8.66^2)=6.24963
# z1/v2 is 5/6.24963 = 0.8
one_run <- function(iter, population, size) {
sample_df <- sample_n(tbl = population, size)
# Calculate the OLS coef. (using unweighted variables)
coef_ols <- b_ols(
y = to_matrix(sample_df, "y_b"),
X = to_matrix(sample_df, c("i", "x1", "x2")))
# Create a data.frame to return
coef_df <- data.frame(
est    = as.vector(coef_ols),
param  = c("int", "b1", "b2"),
iter   = iter
)
# Return the data.frame
print(coef_ols)
return(coef_df)
}
# repeating for other sample sizes:
n_list<-c(10, 100, 1000, 10000, 20000)
for (n in n_list) {
sim_df <- parLapply(
cl = cl,
X = 1:1000,
fun = one_run,
population = data_df, size=n) %>% bind_rows() %>% tbl_df()
# Plotting ----
# Plot a separate histogram of the estimated β2’s for each sample size:
plot_data<-subset(sim_df,  param == "b2")
plot<-ggplot(plot_data, aes(est)) +
geom_histogram(aes(y = ..density..), binwidth = 1/(n^(2/3))) +
stat_function(fun = dnorm, args = list(mean = -2, sd = .8/sqrt(n))) +
xlab("Estimate of B2") +
ylab("Density") +
ggtitle(paste("Sampling distribution for n =", as.character(n)))+
tarduno_theme
plot
print(plot)
}
# Overlay a density of what the theoretical distribution of β2 should look like. What do you notice?
# very similar to the part above -- still asymptotically normally distributed
# The speed of convergence, though, is different -- At 100 is seems that the uniform if further from the theoretical distribution than is the normal case in part A.
