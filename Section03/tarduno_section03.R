# tarduno section 3 ----

setwd("/Users/matthewtarduno/Desktop/212/Section03")


# Review (more on types, vectors) ----
# Does as.numeric() create integers or doubles? (doubles!)
is.double(as.numeric(1))

is.integer(as.numeric(1))

# Are integers and doubles numeric?
is.numeric(as.double(1))

is.numeric(as.integer(1))


# Define the vector
vec <- 1:5
# Square the elements of the vector
vec2 <- vec^2
# Look at the result
vec2
## [1]  1  4  9 16 25

# misc (knitr, NA types) ----

# this saves things in cache until they are changed -- helps speed up the knitting! 
library(knitr)
opts_chunk$set(cache = T)


# NA: 

# Class of NA
class(NA)

# Class of NA from a vector of numbers 
class(c(1, NA)[2])

# Class of NA from a vector of characters (like a chameleon, I guess)
class(c("hi", NA)[2])

# Class of NA from a vector of logicals
class(c(T, NA)[2])

# Demonstrate is.na()
is.na(NA)
is.na(1)
is.na(T)

# What is NaN? (not a number)
0 / 0

# Is NaN NA? It is... wild! 
is.na(NaN)

# Are NaN and NA identical? No! (because different class)
identical(NA, NaN)

# Are they equal?
NA == NaN

# Create the data frame
test_df <- data.frame(
  x = c(NA, "A", "B", NA, "A"),
  y = c(1:4, NA))
# Print test_df to the screen
test_df

# Subset to x == A
dplyr::filter(test_df, x == "A")

# Subset to x != A (Watch out, this is not true complement, because NA's will not be included here)
dplyr::filter(test_df, x != "A")

# x equal to A or NA
dplyr::filter(test_df, x == "A" | is.na(x))

# This is how you read data in as NA: 
# wb_df <- read_csv(
#   file = "world_bank.csv",
#   na = c("", "NA", ".."))

#removing things

item1<-test_df
item2<-test_df
item3<-test_df

# Remove a single object from memory
rm(item1)
# Remove to (or more) objects from memory
rm(list = c("item2", "item3"))
rm(item2, item3)
# Remove everything from memory
rm(list = ls())


#






# functions ----

# Basic syntax 

# foo <- function(arg1, arg2) {
#   ...
#   return(final_stuff)
# }

# Define the function (named 'triple_prod')
triple_prod <- function(x, y, z) {
  # Take the product of the three arguments
  tmp_prod <- x * y * z
  # Return 'tmp_prod'
  return(tmp_prod)
}

# Test the function
triple_prod(x = 2, y = 3, z = 5)
## [1] 30


# Setup ----
# Options
options(stringsAsFactors = F)
# Packages
library(pacman)
p_load(haven, dplyr)
# Define directories
dir_class <- "/Users/matthewtarduno/Desktop/212/"
dir_sec3 <- paste0(dir_class, "Section01/")

# OLS function ----
cars <- read_dta(file = paste0(dir_sec3, "auto.dta"))

# The function require() is the standard way to have a function make sure a package is loaded. 
# select vs. select_ might be a thing of the past 

b_ols <- function(data, y, X) {
  # Require the 'dplyr' package
  require(dplyr)
  
  # Create the y matrix
  y_data <- data %>%
    # Select y variable data from 'data'
    select_(.dots = y) %>%
    # Convert y_data to matrices
    as.matrix()
  
  # Create the X matrix
  X_data <- data %>%
    # Select X variable data from 'data'
    select_(.dots = X) %>%
    # Add a column of ones to X_data
    mutate_("ones" = 1) %>%
    # Move the intercept column to the front
    select_("ones", .dots = X) %>%
    # Convert X_data to matrices
    as.matrix()
  
  # Calculate beta hat
  beta_hat <- solve(t(X_data) %*% X_data) %*% t(X_data) %*% y_data
  # Change the name of 'ones' to 'intercept'
  rownames(beta_hat) <- c("intercept", X)
  # Return beta_hat
  return(beta_hat)
}

# Testing our OLS function ----
b_ols(cars, "mpg", c("price"))
lm(cars$mpg ~ cars$price)

#Another one
b_ols(data = cars, y = "price", X = c("mpg", "weight"))



# Ceci n'est pas une pipe ----


# Select the variables
tmp_data <- select(cars, price, mpg, weight)
# Summarize the selected variables
summary(tmp_data)

# Pipe 
cars %>% select(price, mpg, weight) %>% summary() #think of this as f(g(x)), or "then"


#comparing to canned regression 
p_load(lfe)

# Run the regression with 'felm'
canned_ols <- felm(formula = price ~ mpg + weight, data = cars)
# Summary of the regression
canned_ols %>% summary()
#also contains a bunch of stuff on top of regression estimates 

b_ols(data = cars, y = "price", X = c("mpg", "weight")) 

#for loops ----

# basic:  

for (i in 1:5) {
  print(paste("Hi", i))
}

# lapply (automated for loops)

# lapply() returns a list of the results generated by FUN for each of the elements of X.

lapply(X = 0:4, FUN = sqrt)

# using with our OLS function 

target_vars <- c("price", "headroom", "trunk", "length", "turn", "displacement", "gear_ratio", "foreign")

# The 'lapply' call (here we wrap out ols function because of hwo the arguments are defined and passed)
# the "i" is everything in X, so for every thing in X, we run a regression with X{i} as the y variable. 
results_list <- lapply(
  X = target_vars,
  FUN = function(i) b_ols(data = cars, y = i, X = c("mpg", "weight"))
)
# The results
results_list


# Cleaning up the results list (into data frame)
results_df <- lapply(X = results_list, FUN = data.frame) %>% bind_cols()
# We lose the row names in the process; add them back
rownames(results_df) <- c("intercept", "mpg", "weight")
# Check out results_df
results_df


# simulation ---- 

# A function to calculate bias
data_baker <- function(sample_n, true_beta) {
  # First generate x from N(0,1)
  x <- rnorm(sample_n)
  # Now the error from N(0,1)
  e <- rnorm(sample_n)
  # Now combine true_beta, x, and e to get y
  y <- true_beta[1] + true_beta[2] * x + e
  # Define the data matrix of independent vars.
  X <- cbind(1, x)
  # Force y to be a matrix
  y <- matrix(y, ncol = 1)
  # Calculate the OLS estimates
  b_ols <- solve(t(X) %*% X) %*% t(X) %*% y
  # Convert b_ols to vector
  b_ols <- b_ols %>% as.vector()
  # Calculate bias, force to 2x1 data.frame()
  the_bias <- (true_beta - b_ols) %>%
    matrix(ncol = 2) %>% data.frame()
  # Set names
  names(the_bias) <- c("bias_intercept", "bias_x")
  # Return the bias
  return(the_bias)
}

# Set seed
set.seed(12345)
# Run once
data_baker(sample_n = 100, true_beta = c(1, 3))

# stopped part way through simulation 2.08.2018


# A function to run the simulation
bias_simulator <- function(n_sims, sample_n, true_beta) {
  
  # A function to calculate bias
  data_baker <- function(sample_n, true_beta) {
    # First generate x from N(0,1)
    x <- rnorm(sample_n)
    # Now the error from N(0,1)
    e <- rnorm(sample_n)
    # Now combine true_beta, x, and e to get y
    y <- true_beta[1] + true_beta[2] * x + e
    # Define the data matrix of independent vars.
    X <- cbind(1, x)
    # Force y to be a matrix
    y <- matrix(y, ncol = 1)
    # Calculate the OLS estimates
    b_ols <- solve(t(X) %*% X) %*% t(X) %*% y
    # Convert b_ols to vector
    b_ols <- b_ols %>% as.vector()
    # Calculate bias, force to 2x1 data.frame()
    the_bias <- (true_beta - b_ols) %>%
      matrix(ncol = 2) %>% data.frame()
    # Set names
    names(the_bias) <- c("bias_intercept", "bias_x")
    # Return the bias
    return(the_bias)
  }
  
  # Run data_baker() n_sims times with given parameters
  sims_dt <- lapply(
    X = 1:n_sims,
    FUN = function(i) data_baker(sample_n, true_beta)) %>%
    # Bind the rows together to output a nice data.frame
    bind_rows()
  
  # Return sim_dt
  return(sims_dt)
}


# Set seed
set.seed(12345)
# Run it
sim_dt <- bias_simulator(n_sims = 1e4, sample_n = 100, true_beta = c(1,3))
# Check the results with a histogram
hist(sim_dt[,2],
     breaks = 30,
     main = "Is OLS unbiased?",
     xlab = "Bias")
# Emphasize the zero line
abline(v = 0, col = "blue", lwd = 3)




